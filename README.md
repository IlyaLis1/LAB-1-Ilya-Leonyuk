# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
3 ЛАБОРАТОРНАЯ РАБОТА. РАЗРАБОТКА СИСТЕМЫ МАШИННОГО ОБУЧЕНИЯ

Отчет по лабораторной работе #3 выполнил:
- Леонюк Илья Сергеевич
- РИ000024
Отметка о выполнении заданий:

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.


Структура отчета

- Данные и цель работы: Настройка работы в GitHub, Python, UNity. Ознакомиться с основными операторами зыка Python на
примере реализации линейной регрессии. Леонюк Илья Сергеевич, Школа X, 3 курс. 
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
Поставленная задача: Реализовать систему машинного обучения и интегрировать ее в Unity.
Я скачал все необходимые файлы и добавил их в новый проект Unity. 
Далее запустил Anaconda Prompt и через кансоль активировал нового ML агента и загрузил все необходимые библиотеки.
Далее создал сцену и подключил C# скрипт-файл с кодом.
Подключил к 'Сфере' все необходимые компоненты.
Далее добавил файл конфигурации нейронной сети.
Затем запустил сцену и проверил работу ML Агента (скриншот).
<img width="1080" alt="1" src="https://user-images.githubusercontent.com/115017779/196976492-4d4ca6df-0bb8-4760-8724-c79d09df9147.png">
Обучение проходило успешно и я сделал 9 копий модели «Плоскость-Сфера-Куб» (скриншот).
<img width="1080" alt="2" src="https://user-images.githubusercontent.com/115017779/196976719-7fc170d0-5b08-4a42-8a8d-a3fb69726470.png">
Все также проходило успешно и я создал 27 копий модели «Плоскость-Сфера-Куб» (скриншот).
<img width="1080" alt="3" src="https://user-images.githubusercontent.com/115017779/196976831-51128f81-2c8d-4cea-9d63-b9ea367e2924.png">
<img width="1080" alt="4" src="https://user-images.githubusercontent.com/115017779/196976848-473a5450-48f9-4509-999f-271724fbab46.png">

В конце я проверил работу модели после обучения (скриншот). 
<img width="1080" alt="5" src="https://user-images.githubusercontent.com/115017779/196977062-2ea0aac6-08c5-4947-9b8f-eb7ca186345c.png">
<img width="1080" alt="6" src="https://user-images.githubusercontent.com/115017779/196977086-358f50ed-21bc-4dd8-a641-e8c5db176030.png">




## Задание 2
-gamma 
Соответствует коэффициенту дисконтирования будущих вознаграждений. Это можно рассматривать как то, как далеко в будущем агент должен заботиться о возможных вознаграждениях. В ситуациях, когда агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в отдаленном будущем, это значение должно быть большим. В случаях, когда вознаграждение является более немедленным, оно может быть меньше.


-lambd 
Соответствует параметру лямбда, используемому при расчете обобщенной оценки преимущества (GAE). Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает компромисс между ними, и правильное значение может привести к более стабильному процессу обучения.


-buffer_size
Соответствует тому, сколько опыта (наблюдений агента, действий и полученных вознаграждений) должно быть собрано, прежде чем мы выполним какое-либо обучение или обновление модели. Это должно быть кратно batch_size. Обычно больший размер буфера соответствует более стабильным обновлениям обучения.



-batch_size
Это количество опытов, используемых для одной итерации обновления градиентного спуска. Это всегда должно быть частью buffer_size. Если вы используете непрерывное пространство действий, это значение должно быть большим (порядка 1000). Если вы используете дискретное пространство действий, это значение должно быть меньше (порядка 10 с).


-num_epoch
Количество проходов через буфер опыта во время градиентного спуска. Чем больше размер batch_size, тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.


-Learning_rate
Соответствует мощности каждого шага обновления градиентного спуска. Обычно это значение следует уменьшать, если обучение нестабильно, а вознаграждение не увеличивается постоянно.


-time_horizon
Соответствует тому, сколько шагов опыта необходимо собрать для каждого агента, прежде чем добавить его в буфер опыта. Когда этот предел достигается до конца эпизода, оценка значения используется для прогнозирования общего ожидаемого вознаграждения из текущего состояния агента. Таким образом, этот параметр является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (длинный временной горизонт) и более предвзятой, но менее разнообразной оценкой (короткий временной горизонт). В тех случаях, когда в эпизоде ​​есть частые награды или эпизоды непомерно велики, более идеальным может быть меньшее количество. Это число должно быть достаточно большим, чтобы охватить все важные действия в последовательности действий агента.


-max_steps
Соответствует количеству шагов моделирования (умноженных на пропуск кадров) в процессе обучения. Это значение следует увеличить для более сложных задач.



-beta
Соответствует силе энтропийной регуляризации, что делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard) медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета. Если энтропия падает слишком медленно, уменьшите бета.



-epsilon
Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более стабильным обновлениям, но также замедлит процесс обучения.



-normalize
Соответствует тому, применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на скользящем среднем и дисперсии векторного наблюдения. Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна для более простых задач дискретного управления.

-num_layers
Соответствует тому, сколько скрытых слоев присутствует после ввода наблюдения или после кодирования CNN визуального наблюдения. Для простых задач меньше слоев, скорее всего, будут обучать быстрее и эффективнее. Для более сложных задач управления может потребоваться больше слоев.

-Behavior Parameters
У каждого агента должно быть поведение. Поведение определяет, как агент принимает решения.

-Decision Requester
Компонент предоставляет удобный и гибкий способ инициировать процесс принятия решения агентом.

## Задание 3
-

## Выводы
После проделанной работы, я пришел к выводу, что я познакомился с реализации системы машинного обучения и интеграции ее в Unity. 
Определенно точно можно сказать, что при создании больше количества копий модели «Плоскость-Сфера-Куб», обучение происходит быстрее. 
Конечно, нагрузка на процессор (в данной лабораторной работе мы использовали именно ресурсы CPU) повышается. Но и результаты становятся лучше и точнее. 
